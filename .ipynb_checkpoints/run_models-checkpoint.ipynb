{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of all books combined:  72569456\n",
      "size of data:  5000000\n",
      "number of unique characters:  48\n",
      "sentence_length:  50\n",
      "num unique_chars:  48\n",
      "['enly awaywhich mrs. jennings was desirous of her f', 'awaywhich mrs. jennings was desirous of her feelin', 'hich mrs. jennings was desirous of her feeling; fo', 'mrs. jennings was desirous of her feeling; for bes', 'jennings was desirous of her feeling; for besides ']\n",
      "['e', 'g', 'r', 'i', 't']\n",
      "(999990, 50, 48)\n",
      "(999990, 48)\n"
     ]
    }
   ],
   "source": [
    "books_combined = open('books_combined.txt').read()\n",
    "print('length of all books combined: ', len(books_combined))\n",
    "\n",
    "#work with a subset of the books\n",
    "start_book_ind = 15000000\n",
    "end_book_ind = 20000000\n",
    "sub_books_combined = books_combined[start_book_ind: end_book_ind]\n",
    "print('size of data: ', len(sub_books_combined))\n",
    "\n",
    "char_to_int = {}\n",
    "int_to_char = {}\n",
    "#turn characters into integers and integers into characters - chars should be sorted to keep consistency\n",
    "for i, char in enumerate(sorted(set(sub_books_combined))):\n",
    "    char_to_int[char] = i\n",
    "    int_to_char[i] = char\n",
    "    \n",
    "print('number of unique characters: ', len(char_to_int)) #should be 48 every time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#returns a tensor for training\n",
    "def get_train(book, sentence_length=50):\n",
    "    sent = []\n",
    "    next_c = []\n",
    "    #get varying sentences of similar sizes - step for a size of 5 - 5 is an arbitraily chosen number\n",
    "    for i in range(0, len(book)-sentence_length, 5):\n",
    "        begin_char = i\n",
    "        end_char = i+sentence_length\n",
    "        #combine sentence to a sentence list\n",
    "        sent.append(book[begin_char: end_char])\n",
    "        #get the next character after the sentence\n",
    "        next_c.append(book[end_char])\n",
    "\n",
    "    print(sent[:5])\n",
    "    print(next_c[:5])\n",
    "\n",
    "    #turn into Tensors for training the model - using one hot encoding method\n",
    "    # 1 if char in sentence and 0 otherwise\n",
    "    unique_c_length = len(set(book)) # number of unique characters in text\n",
    "    num_sent = (len(sent)) #numer of sentences created\n",
    "    x = np.zeros((num_sent, sentence_length, unique_c_length), dtype=np.int8)\n",
    "    y = np.zeros((num_sent, unique_c_length), dtype=np.int8)\n",
    "\n",
    "    for i, sentence in enumerate(sent):\n",
    "        for k, c in enumerate(sentence):\n",
    "            c_ind = char_to_int[c]\n",
    "            x[i, k, c_ind] = 1\n",
    "\n",
    "        next_c_ind = char_to_int[next_c[i]]\n",
    "        y[i, next_c_ind] = 1\n",
    "\n",
    "\n",
    "    return x,y\n",
    "  \n",
    "  \n",
    "  \n",
    "\n",
    "sentence_length = 50\n",
    "num_unique_chars = len(set(sub_books_combined))\n",
    "print(\"sentence_length: \", sentence_length)\n",
    "print(\"num unique_chars: \", num_unique_chars)\n",
    "\n",
    "#create tensors\n",
    "x, y = get_train(sub_books_combined, sentence_length=sentence_length)\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/38\n",
      "999936/999990 [============================>.] - ETA: 0s - loss: 1.5092\n",
      "Epoch 00001: loss improved from inf to 1.50920, saving model to deeper-network-weights-improvement-01-1.5092-bigger.hdf5\n",
      "999990/999990 [==============================] - 1111s 1ms/sample - loss: 1.5092\n",
      "Epoch 2/38\n",
      "999936/999990 [============================>.] - ETA: 0s - loss: 1.4966\n",
      "Epoch 00002: loss improved from 1.50920 to 1.49657, saving model to deeper-network-weights-improvement-02-1.4966-bigger.hdf5\n",
      "999990/999990 [==============================] - 1068s 1ms/sample - loss: 1.4966\n",
      "Epoch 3/38\n",
      "999936/999990 [============================>.] - ETA: 0s - loss: 1.4906\n",
      "Epoch 00003: loss improved from 1.49657 to 1.49055, saving model to deeper-network-weights-improvement-03-1.4906-bigger.hdf5\n",
      "999990/999990 [==============================] - 1078s 1ms/sample - loss: 1.4906\n",
      "Epoch 4/38\n",
      "999936/999990 [============================>.] - ETA: 0s - loss: 1.4861\n",
      "Epoch 00004: loss improved from 1.49055 to 1.48611, saving model to deeper-network-weights-improvement-04-1.4861-bigger.hdf5\n",
      "999990/999990 [==============================] - 1111s 1ms/sample - loss: 1.4861\n",
      "Epoch 5/38\n",
      "999936/999990 [============================>.] - ETA: 0s - loss: 1.4836\n",
      "Epoch 00005: loss improved from 1.48611 to 1.48357, saving model to deeper-network-weights-improvement-05-1.4836-bigger.hdf5\n",
      "999990/999990 [==============================] - 1130s 1ms/sample - loss: 1.4836\n",
      "Epoch 6/38\n",
      "999936/999990 [============================>.] - ETA: 0s - loss: 1.4801\n",
      "Epoch 00006: loss improved from 1.48357 to 1.48014, saving model to deeper-network-weights-improvement-06-1.4801-bigger.hdf5\n",
      "999990/999990 [==============================] - 1132s 1ms/sample - loss: 1.4801\n",
      "Epoch 7/38\n",
      "999936/999990 [============================>.] - ETA: 0s - loss: 1.4781\n",
      "Epoch 00007: loss improved from 1.48014 to 1.47814, saving model to deeper-network-weights-improvement-07-1.4781-bigger.hdf5\n",
      "999990/999990 [==============================] - 1128s 1ms/sample - loss: 1.4781\n",
      "Epoch 8/38\n",
      "999936/999990 [============================>.] - ETA: 0s - loss: 1.4759\n",
      "Epoch 00008: loss improved from 1.47814 to 1.47587, saving model to deeper-network-weights-improvement-08-1.4759-bigger.hdf5\n",
      "999990/999990 [==============================] - 1132s 1ms/sample - loss: 1.4759\n",
      "Epoch 9/38\n",
      "999936/999990 [============================>.] - ETA: 0s - loss: 1.4738\n",
      "Epoch 00009: loss improved from 1.47587 to 1.47377, saving model to deeper-network-weights-improvement-09-1.4738-bigger.hdf5\n",
      "999990/999990 [==============================] - 1151s 1ms/sample - loss: 1.4738\n",
      "Epoch 10/38\n",
      "999936/999990 [============================>.] - ETA: 0s - loss: 1.4721\n",
      "Epoch 00010: loss improved from 1.47377 to 1.47208, saving model to deeper-network-weights-improvement-10-1.4721-bigger.hdf5\n",
      "999990/999990 [==============================] - 1121s 1ms/sample - loss: 1.4721\n",
      "Epoch 11/38\n",
      "999936/999990 [============================>.] - ETA: 0s - loss: 1.4696\n",
      "Epoch 00011: loss improved from 1.47208 to 1.46962, saving model to deeper-network-weights-improvement-11-1.4696-bigger.hdf5\n",
      "999990/999990 [==============================] - 1119s 1ms/sample - loss: 1.4696\n",
      "Epoch 12/38\n",
      "999936/999990 [============================>.] - ETA: 0s - loss: 1.4686\n",
      "Epoch 00012: loss improved from 1.46962 to 1.46863, saving model to deeper-network-weights-improvement-12-1.4686-bigger.hdf5\n",
      "999990/999990 [==============================] - 1124s 1ms/sample - loss: 1.4686\n",
      "Epoch 13/38\n",
      "999936/999990 [============================>.] - ETA: 0s - loss: 1.4664\n",
      "Epoch 00013: loss improved from 1.46863 to 1.46638, saving model to deeper-network-weights-improvement-13-1.4664-bigger.hdf5\n",
      "999990/999990 [==============================] - 1122s 1ms/sample - loss: 1.4664\n",
      "Epoch 14/38\n",
      "999936/999990 [============================>.] - ETA: 0s - loss: 1.4656\n",
      "Epoch 00014: loss improved from 1.46638 to 1.46566, saving model to deeper-network-weights-improvement-14-1.4657-bigger.hdf5\n",
      "999990/999990 [==============================] - 1121s 1ms/sample - loss: 1.4657\n",
      "Epoch 15/38\n",
      "999936/999990 [============================>.] - ETA: 0s - loss: 1.4646\n",
      "Epoch 00015: loss improved from 1.46566 to 1.46466, saving model to deeper-network-weights-improvement-15-1.4647-bigger.hdf5\n",
      "999990/999990 [==============================] - 1096s 1ms/sample - loss: 1.4647\n",
      "Epoch 16/38\n",
      "999936/999990 [============================>.] - ETA: 0s - loss: 1.4628\n",
      "Epoch 00016: loss improved from 1.46466 to 1.46284, saving model to deeper-network-weights-improvement-16-1.4628-bigger.hdf5\n",
      "999990/999990 [==============================] - 1066s 1ms/sample - loss: 1.4628\n",
      "Epoch 17/38\n",
      "999936/999990 [============================>.] - ETA: 0s - loss: 1.4616\n",
      "Epoch 00017: loss improved from 1.46284 to 1.46165, saving model to deeper-network-weights-improvement-17-1.4616-bigger.hdf5\n",
      "999990/999990 [==============================] - 1045s 1ms/sample - loss: 1.4616\n",
      "Epoch 18/38\n",
      "999936/999990 [============================>.] - ETA: 0s - loss: 1.4611\n",
      "Epoch 00018: loss improved from 1.46165 to 1.46114, saving model to deeper-network-weights-improvement-18-1.4611-bigger.hdf5\n",
      "999990/999990 [==============================] - 1051s 1ms/sample - loss: 1.4611\n",
      "Epoch 19/38\n",
      "999936/999990 [============================>.] - ETA: 0s - loss: 1.4605\n",
      "Epoch 00019: loss improved from 1.46114 to 1.46049, saving model to deeper-network-weights-improvement-19-1.4605-bigger.hdf5\n",
      "999990/999990 [==============================] - 1050s 1ms/sample - loss: 1.4605\n",
      "Epoch 20/38\n",
      "999936/999990 [============================>.] - ETA: 0s - loss: 1.4593\n",
      "Epoch 00020: loss improved from 1.46049 to 1.45931, saving model to deeper-network-weights-improvement-20-1.4593-bigger.hdf5\n",
      "999990/999990 [==============================] - 1047s 1ms/sample - loss: 1.4593\n",
      "Epoch 21/38\n",
      "999936/999990 [============================>.] - ETA: 0s - loss: 1.4585\n",
      "Epoch 00021: loss improved from 1.45931 to 1.45849, saving model to deeper-network-weights-improvement-21-1.4585-bigger.hdf5\n",
      "999990/999990 [==============================] - 1044s 1ms/sample - loss: 1.4585\n",
      "Epoch 22/38\n",
      "999936/999990 [============================>.] - ETA: 0s - loss: 1.4575\n",
      "Epoch 00022: loss improved from 1.45849 to 1.45752, saving model to deeper-network-weights-improvement-22-1.4575-bigger.hdf5\n",
      "999990/999990 [==============================] - 1047s 1ms/sample - loss: 1.4575\n",
      "Epoch 23/38\n",
      "999936/999990 [============================>.] - ETA: 0s - loss: 1.4560\n",
      "Epoch 00023: loss improved from 1.45752 to 1.45602, saving model to deeper-network-weights-improvement-23-1.4560-bigger.hdf5\n",
      "999990/999990 [==============================] - 1004s 1ms/sample - loss: 1.4560\n",
      "Epoch 24/38\n",
      "999936/999990 [============================>.] - ETA: 0s - loss: 1.4555\n",
      "Epoch 00024: loss improved from 1.45602 to 1.45551, saving model to deeper-network-weights-improvement-24-1.4555-bigger.hdf5\n",
      "999990/999990 [==============================] - 1003s 1ms/sample - loss: 1.4555\n",
      "Epoch 25/38\n",
      "999936/999990 [============================>.] - ETA: 0s - loss: 1.4551\n",
      "Epoch 00025: loss improved from 1.45551 to 1.45509, saving model to deeper-network-weights-improvement-25-1.4551-bigger.hdf5\n",
      "999990/999990 [==============================] - 1004s 1ms/sample - loss: 1.4551\n",
      "Epoch 26/38\n",
      "999936/999990 [============================>.] - ETA: 0s - loss: 1.4538\n",
      "Epoch 00026: loss improved from 1.45509 to 1.45378, saving model to deeper-network-weights-improvement-26-1.4538-bigger.hdf5\n",
      "999990/999990 [==============================] - 1003s 1ms/sample - loss: 1.4538\n",
      "Epoch 27/38\n",
      "999936/999990 [============================>.] - ETA: 0s - loss: 1.4529\n",
      "Epoch 00027: loss improved from 1.45378 to 1.45290, saving model to deeper-network-weights-improvement-27-1.4529-bigger.hdf5\n",
      "999990/999990 [==============================] - 1459s 1ms/sample - loss: 1.4529\n",
      "Epoch 28/38\n",
      "999936/999990 [============================>.] - ETA: 0s - loss: 1.4530\n",
      "Epoch 00028: loss did not improve from 1.45290\n",
      "999990/999990 [==============================] - 1292s 1ms/sample - loss: 1.4530\n",
      "Epoch 29/38\n",
      "999936/999990 [============================>.] - ETA: 0s - loss: 1.4515\n",
      "Epoch 00029: loss improved from 1.45290 to 1.45153, saving model to deeper-network-weights-improvement-29-1.4515-bigger.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999990/999990 [==============================] - 1017s 1ms/sample - loss: 1.4515\n",
      "Epoch 30/38\n",
      "999936/999990 [============================>.] - ETA: 0s - loss: 1.4519\n",
      "Epoch 00030: loss did not improve from 1.45153\n",
      "999990/999990 [==============================] - 971s 971us/sample - loss: 1.4519\n",
      "Epoch 31/38\n",
      "999936/999990 [============================>.] - ETA: 0s - loss: 1.4515\n",
      "Epoch 00031: loss improved from 1.45153 to 1.45151, saving model to deeper-network-weights-improvement-31-1.4515-bigger.hdf5\n",
      "999990/999990 [==============================] - 982s 982us/sample - loss: 1.4515\n",
      "Epoch 32/38\n",
      "999936/999990 [============================>.] - ETA: 0s - loss: 1.4508\n",
      "Epoch 00032: loss improved from 1.45151 to 1.45083, saving model to deeper-network-weights-improvement-32-1.4508-bigger.hdf5\n",
      "999990/999990 [==============================] - 989s 989us/sample - loss: 1.4508\n",
      "Epoch 33/38\n",
      "999936/999990 [============================>.] - ETA: 0s - loss: 1.4497\n",
      "Epoch 00033: loss improved from 1.45083 to 1.44967, saving model to deeper-network-weights-improvement-33-1.4497-bigger.hdf5\n",
      "999990/999990 [==============================] - 1882s 2ms/sample - loss: 1.4497\n",
      "Epoch 34/38\n",
      "999936/999990 [============================>.] - ETA: 0s - loss: 1.4493\n",
      "Epoch 00034: loss improved from 1.44967 to 1.44929, saving model to deeper-network-weights-improvement-34-1.4493-bigger.hdf5\n",
      "999990/999990 [==============================] - 1132s 1ms/sample - loss: 1.4493\n",
      "Epoch 35/38\n",
      "999936/999990 [============================>.] - ETA: 0s - loss: 1.4484\n",
      "Epoch 00035: loss improved from 1.44929 to 1.44845, saving model to deeper-network-weights-improvement-35-1.4485-bigger.hdf5\n",
      "999990/999990 [==============================] - 1081s 1ms/sample - loss: 1.4485\n",
      "Epoch 36/38\n",
      "999936/999990 [============================>.] - ETA: 0s - loss: 1.4483\n",
      "Epoch 00036: loss improved from 1.44845 to 1.44827, saving model to deeper-network-weights-improvement-36-1.4483-bigger.hdf5\n",
      "999990/999990 [==============================] - 1091s 1ms/sample - loss: 1.4483\n",
      "Epoch 37/38\n",
      "999936/999990 [============================>.] - ETA: 0s - loss: 1.4481\n",
      "Epoch 00037: loss improved from 1.44827 to 1.44810, saving model to deeper-network-weights-improvement-37-1.4481-bigger.hdf5\n",
      "999990/999990 [==============================] - 1183s 1ms/sample - loss: 1.4481\n",
      "Epoch 38/38\n",
      "999936/999990 [============================>.] - ETA: 0s - loss: 1.4468\n",
      "Epoch 00038: loss improved from 1.44810 to 1.44676, saving model to deeper-network-weights-improvement-38-1.4468-bigger.hdf5\n",
      "999990/999990 [==============================] - 1224s 1ms/sample - loss: 1.4468\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x12fc91fd0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save models that perform best\n",
    "filepath=\"deeper-network-weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "#create network\n",
    "model = Sequential()\n",
    "#128 nodes are used b/c the current network is small\n",
    "model.add(LSTM(128, input_shape=(sentence_length, num_unique_chars)))\n",
    "model.add(Dropout(0.2))\n",
    "# model.add(LSTM(128))\n",
    "# model.add(Dropout(0.2))\n",
    "#add the output layer\n",
    "model.add(Dense(num_unique_chars, activation='softmax'))\n",
    "#load previous weights\n",
    "weights_filename = 'models/round4/weights-improvement-18-1.4762-bigger.hdf5'\n",
    "model.load_weights(weights_filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "model.fit(x, y, epochs=38, batch_size=128, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates new sentence\n",
    "def create_passage(model):\n",
    "    #complete passage string\n",
    "    gen_sentence = ''\n",
    "\n",
    "    #create sentence\n",
    "    start_ind = random.randint(0, len(sub_books_combined) - sentence_length - 1)\n",
    "    sentence = sub_books_combined[start_ind: start_ind+sentence_length]\n",
    "    gen_sentence += sentence\n",
    "    print('original sentenc: ', sentence)\n",
    "\n",
    "    for i in range(500):\n",
    "    #turn sentence into model format\n",
    "        x_pred = np.zeros((1, sentence_length, len(char_to_int)))\n",
    "        for k, c in enumerate(sentence):\n",
    "            x_ind = char_to_int[c]\n",
    "            x_pred[0, k, x_ind] = 1\n",
    "\n",
    "        #predict next character - returns predicted probabilities\n",
    "        prob_c = model.predict(x_pred, verbose=0)[0]\n",
    "        #turn to float64 - mulitnomial gives error otherwise\n",
    "        prob_c = np.asarray(prob_c).astype('float64')\n",
    "        #     print(prob_c)\n",
    "        #sample from the probability \n",
    "        log_prob = np.log(prob_c) / 0.5\n",
    "        #     print(log_prob)\n",
    "        exp_prob = np.exp(log_prob)\n",
    "        #     print(exp_prob)\n",
    "        pred_prob = exp_prob/np.sum(exp_prob)\n",
    "        #     print(pred_prob)\n",
    "        p = np.random.multinomial(1, pred_prob, 1) \n",
    "        #     print(p)\n",
    "        prob_ind = np.argmax(p)\n",
    "        #     print(prob_ind)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #turn int to character\n",
    "        next_c = int_to_char[prob_ind]\n",
    "        #add character to generated sentence\n",
    "        gen_sentence += next_c\n",
    "        #     print(gen_sentence)\n",
    "        #get new sentence by sliding to index of sentence\n",
    "        sentence = sentence[1:]+next_c\n",
    "\n",
    "\n",
    "    return gen_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original sentenc:  sly          stirred us to glory and gave me these\n",
      "sly          stirred us to glory and gave me these of the sense of the strong and being in a state and the street the soul for the spoken to scarne the fire destroyed and with the subjects best notice and absolute stream by the death of a contrary of the concerning the supportions is been ever known to the interest of the carriage in the man project gutenberg-tm electronic works in a most common-wealth and one of one final and the more not one of the wants of the consequence where the present stood the other were the small and project gutenberg\n"
     ]
    }
   ],
   "source": [
    "#test out creating a passage using some of the passage within the books\n",
    "print(create_passage(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model_4_architecture.json\", \"w+\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model_4.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = model.history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_dict['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(38), history_dict['loss'])\n",
    "plt.title('Training Data Loss Function')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json and create model architecture\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate loaded model on test data/ predict\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(X, Y, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
