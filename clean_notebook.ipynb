{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of books used to create corpus:  96\n",
      "Number of characters in corpus:  74454677\n",
      "Number of unique words:  343646\n"
     ]
    }
   ],
   "source": [
    "#Read all Text of Books \n",
    "print('Number of books used to create corpus: ', 96)\n",
    "books_combined = open('books_combined.txt').read()\n",
    "print('Number of characters in corpus: ', len(books_combined))\n",
    "\n",
    "unique_words = len(set(books_combined.split(' ')))\n",
    "print('Number of unique words: ', unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ven and a half pounds since i saw you. seven! i answered. indeed, i should have thought a little more. just a trifle more, i fancy, watson. and in practice again, i observe. you did not tell me that you intended to go into harness. then, how do you know? i see it, i deduce it. how do i know that you have been getting yourself very wet lately, and that you have a most clumsy and careless servant girl? my dear holmes, said i, this is too much. you would certainly have been burned, had you lived a few centuries ago. it is true that i had a country walk on thursday and came home in a dreadful mess, but as i have changed my clothes i cant imagine how you deduce it. as to mary jane, she is incorrigible, and my wife has given her notice, but there, again, i fail to see how you work it out. he chuckled to himself and rubbed his long, nervous hands together. it is simplicity itself, said he; my eyes tell me that on the inside of your left shoe, just where the firelight strikes it, the leather i'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_combined[5000:6000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique characters:  48\n",
      "The unique characters within the text:  [' ', '!', '&', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "#data is too big, kernel can't handle, get a subset\n",
    "start_book_ind = 20000000\n",
    "end_book_ind = 25000000\n",
    "sub_books_combined = books_combined[start_book_ind: end_book_ind]\n",
    "\n",
    "char_to_int = {}\n",
    "int_to_char = {}\n",
    "#turn characters into integers and integers into characters - chars should be sorted to keep consistency\n",
    "for i, char in enumerate(sorted(set(sub_books_combined))):\n",
    "    char_to_int[char] = i\n",
    "    int_to_char[i] = char\n",
    "    \n",
    "print('number of unique characters: ', len(char_to_int)) #should be 48 every time\n",
    "print('The unique characters within the text: ', list(char_to_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns a tensor for training\n",
    "def get_train(book, sentence_length=50):\n",
    "    sent = []\n",
    "    next_c = []\n",
    "    #get varying sentences of similar sizes - step for a size of 5 - 5 is an arbitraily chosen number\n",
    "    for i in range(0, len(book)-sentence_length, 2):\n",
    "        begin_char = i\n",
    "        end_char = i+sentence_length\n",
    "        #combine sentence to a sentence list\n",
    "        sent.append(book[begin_char: end_char])\n",
    "        #get the next character after the sentence\n",
    "        next_c.append(book[end_char])\n",
    "\n",
    "    print(sent[:5])\n",
    "    print(next_c[:5])\n",
    "\n",
    "    #turn into Tensors for training the model - using one hot encoding method\n",
    "    # 1 if char in sentence and 0 otherwise\n",
    "    unique_c_length = len(set(book)) # number of unique characters in text\n",
    "    num_sent = (len(sent)) #numer of sentences created\n",
    "    x = np.zeros((num_sent, sentence_length, unique_c_length), dtype=np.int8)\n",
    "    y = np.zeros((num_sent, unique_c_length), dtype=np.int8)\n",
    "\n",
    "    for i, sentence in enumerate(sent):\n",
    "        for k, c in enumerate(sentence):\n",
    "            c_ind = char_to_int[c]\n",
    "            x[i, k, c_ind] = 1\n",
    "\n",
    "        next_c_ind = char_to_int[next_c[i]]\n",
    "        y[i, next_c_ind] = 1\n",
    "\n",
    "\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence_length:  50\n",
      "num unique_chars:  48\n",
      "['force down the general throat like a bolus, always', 'rce down the general throat like a bolus, always t', 'e down the general throat like a bolus, always to ', 'down the general throat like a bolus, always to be', 'wn the general throat like a bolus, always to be h']\n",
      "[' ', 'o', 'b', ' ', 'e']\n",
      "(2499975, 50, 48)\n",
      "(2499975, 48)\n"
     ]
    }
   ],
   "source": [
    "#set the length of each sentence\n",
    "sentence_length = 50\n",
    "num_unique_chars = len(set(sub_books_combined))\n",
    "print(\"sentence_length: \", sentence_length)\n",
    "print(\"num unique_chars: \", num_unique_chars)\n",
    "\n",
    "#create tensors\n",
    "x, y = get_train(sub_books_combined, sentence_length=sentence_length)\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e same principles, like so many pianoforte legs. he had been put through an immense variety of paces, and had answered volumes of head-breaking questions. orthography, etymology, syntax, and prosody, biography, astronomy, geography, and general cosmography, the sciences of compound proportion, algebra, land-surveying and levelling, vocal music, and drawing from models, were all at the ends of his ten chilled fingers. he had worked his stony way into her majestys most honourable privy councils schedule b, and had taken the bloom off the higher branches of mathematics and physical science, french, german, latin, and greek. he knew all about all the water sheds of all the world (whatever they are), and all the histories of all the peoples, and all the names of all the rivers and mountains, and all the productions, manners, and customs of all the countries, and all their boundaries and bearings on the two and thirty points of the compass. ah, rather overdone, mchoakumchild. if he had only '"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_books_combined[5000:6000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save models that perform best\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "#create network\n",
    "model = Sequential()\n",
    "#128 nodes are used b/c the current network is small\n",
    "model.add(LSTM(128, input_shape=(sentence_length, num_unique_chars)))\n",
    "model.add(Dropout(0.2))\n",
    "# model.add(LSTM(128))\n",
    "# model.add(Dropout(0.2))\n",
    "#add the output layer\n",
    "model.add(Dense(num_unique_chars, activation='softmax'))\n",
    "#load previous weights\n",
    "weights_filename = 'models/round4/model_4.h5'\n",
    "model.load_weights(weights_filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "model.fit(x, y, epochs=50, batch_size=256, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates new sentence\n",
    "def create_passage(model):\n",
    "    #complete passage string\n",
    "    gen_sentence = ''\n",
    "\n",
    "    #create sentence\n",
    "    start_ind = random.randint(0, len(sub_books_combined) - sentence_length - 1)\n",
    "    sentence = sub_books_combined[start_ind: start_ind+sentence_length]\n",
    "    gen_sentence += sentence\n",
    "    print('original sentenc: ', sentence)\n",
    "\n",
    "    for i in range(500):\n",
    "    #turn sentence into model format\n",
    "        x_pred = np.zeros((1, sentence_length, len(char_to_int)))\n",
    "        for k, c in enumerate(sentence):\n",
    "            x_ind = char_to_int[c]\n",
    "            x_pred[0, k, x_ind] = 1\n",
    "\n",
    "        #predict next character - returns predicted probabilities\n",
    "        prob_c = model.predict(x_pred, verbose=0)[0]\n",
    "        #turn to float64 - mulitnomial gives error otherwise\n",
    "        prob_c = np.asarray(prob_c).astype('float64')\n",
    "        #     print(prob_c)\n",
    "        #sample from the probability \n",
    "        log_prob = np.log(prob_c) / 0.5\n",
    "        #     print(log_prob)\n",
    "        exp_prob = np.exp(log_prob)\n",
    "        #     print(exp_prob)\n",
    "        pred_prob = exp_prob/np.sum(exp_prob)\n",
    "        #     print(pred_prob)\n",
    "        p = np.random.multinomial(1, pred_prob, 1) \n",
    "        #     print(p)\n",
    "        prob_ind = np.argmax(p)\n",
    "        #     print(prob_ind)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #turn int to character\n",
    "        next_c = int_to_char[prob_ind]\n",
    "        #add character to generated sentence\n",
    "        gen_sentence += next_c\n",
    "        #     print(gen_sentence)\n",
    "        #get new sentence by sliding to index of sentence\n",
    "        sentence = sentence[1:]+next_c\n",
    "\n",
    "\n",
    "    return gen_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test out creating a passage using some of the passage within the books\n",
    "print(create_passage(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "original sentenc: sly stirred us to glory and gave me these<br>\n",
    "sly stirred us to glory and gave me these of the sense of the strong and being in a state and the street the soul for the spoken to scarne the fire destroyed and with the subjects best notice and absolute stream by the death of a contrary of the concerning the supportions is been ever known to the interest of the carriage in the man project gutenberg-tm electronic works in a most common-wealth and one of one final and the more not one of the wants of the consequence where the present stood the other were the small and project gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model_4_architecture.json\", \"w+\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model_4.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saved model to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get loss \n",
    "history_dict = model.history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the loss function\n",
    "plt.plot(range(38), history_dict['loss'])\n",
    "plt.title('Training Data Loss Function')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
